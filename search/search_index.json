{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mimicry","text":"<p>Mimicry is a Python-based tool designed for generating and streaming synthetic data. It allows users to define data schemas, generate mock data based on these schemas using the powerful mimesis and polars libraries, and stream this data to various sinks like Delta Lake, DuckDB, PostgreSQL, Apache Kafka, or Apache Iceberg. Additionally, Mimicry can expose the generated data through a FastAPI-based web server.</p> <p>Early Stage Project</p> <p>This project is in its early stages and is not yet production-ready. It is intended for testing and development purposes only. Use at your own risk.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>The primary purpose of Mimicry is to provide developers and data engineers with a flexible and easy-to-use tool for:</p> <ul> <li>Generating realistic mock data: For testing, development, and demonstration purposes.</li> <li>Simulating data streams: To test data pipelines and stream processing applications.</li> <li>Providing mock APIs: For frontend development or microservice testing when backend services are not yet ready.</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Schema-based Data Generation: Define the structure and type of your data using simple YAML configuration files. See Configuration.</li> <li>Rich Data Types: Leverages the mimesis library for a wide variety of data field types (names, addresses, numbers, dates, custom patterns, etc.).</li> <li>Multiple Sinks: Stream generated data to different storage solutions. See Supported Sinks.</li> <li>Delta Lake: For robust, transactional data lake storage.</li> <li>DuckDB: For fast, embedded analytical database operations.</li> <li>PostgreSQL: For reliable relational database storage.</li> <li>Apache Kafka: For high-throughput, distributed event streaming.</li> <li>Apache Iceberg: For managing large, analytic datasets with a table format.</li> <li>Data Streaming: Continuously generate and append data in batches at specified intervals. See the CLI documentation.</li> <li>API Server: Automatically build and run a FastAPI web server to expose data generation endpoints. See the CLI documentation.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Navigate through the documentation using the menu to learn more about Configuration, Supported Sinks, and the Command-Line Interface (CLI).</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#030-2025-07-10","title":"0.3.0 (2025-07-10)","text":""},{"location":"CHANGELOG/#feature","title":"Feature","text":"<ul> <li>add support for configuration files (sinks and tableconfigurations) stored in Object Storage. (#5)</li> </ul>"},{"location":"CHANGELOG/#021-2025-06-22","title":"0.2.1 (2025-06-22)","text":""},{"location":"CHANGELOG/#feature_1","title":"Feature","text":"<ul> <li>add documentation for GCS and S3 sinks, and enhance CLI options. (#4)</li> </ul>"},{"location":"CHANGELOG/#020-2025-06-17","title":"0.2.0 (2025-06-17)","text":""},{"location":"CHANGELOG/#feature_2","title":"Feature","text":"<ul> <li>initial version.</li> </ul>"},{"location":"cli/","title":"Command-Line Interface (CLI)","text":"<p>Mimicry provides a CLI for its main functionalities, built using Typer. You can access help for any command by appending <code>--help</code>.</p> <p><pre><code>uv run mimicry --help\n</code></pre> Or, if using Docker: <pre><code>docker run -it --rm k0wojtasinski/mimicry:latest uv run mimicry --help\n</code></pre></p>"},{"location":"cli/#generating-and-streaming-data-generate","title":"Generating and Streaming Data (<code>generate</code>)","text":"<p>This command generates data based on a schema and streams it to a configured sink.</p> <pre><code>uv run mimicry generate \\\n    --schema path/to/your/table_config.yaml \\\n    --sink path/to/your/sink_config.yaml \\\n    --count 1000 \\\n    --interval 5 \\\n    --batches 10 \\\n    --strict\n</code></pre> <p>Options for <code>generate</code>:</p> <ul> <li><code>-p</code>, <code>--schema</code> PATH: Path to the table schema configuration YAML file. (Required)</li> <li><code>-s</code>, <code>--sink</code> PATH: Path to the sink configuration YAML file. (Required)</li> <li><code>-c</code>, <code>--count</code> INTEGER: Number of records to generate per batch. (Required)</li> <li><code>-i</code>, <code>--interval</code> INTEGER: Interval in seconds between generating batches. (Required)</li> <li><code>-b</code>, <code>--batches</code> INTEGER: Number of batches to generate. Set to -1 for continuous streaming. (Required)</li> <li><code>-x</code>, <code>--strict</code>: If True, raises an error if the table configuration is invalid. [default: False]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"cli/#running-as-an-api-server-serve","title":"Running as an API Server (<code>serve</code>)","text":"<p>This command starts a FastAPI web server to expose data generation endpoints.</p> <pre><code>uv run mimicry serve \\\n    --schema path/to/your/table_config1.yaml \\\n    --schema path/to/your/table_config2.yaml \\\n    --title \"My Mock API\" \\\n    --description \"An API for generating mock data.\" \\\n    --max-count 1000 \\\n    --strict\n</code></pre> <p>Options for <code>serve</code>:</p> <ul> <li><code>-s</code>, <code>--schema</code> PATH: Path to a table schema configuration YAML file. Can be specified multiple times for multiple tables. (Required)</li> <li><code>-t</code>, <code>--title</code> TEXT: The title of the FastAPI application. [default: Mimicry API]</li> <li><code>-d</code>, <code>--description</code> TEXT: A description of the FastAPI application. [default: Mimicry API for data generation]</li> <li><code>-v</code>, <code>--version</code> TEXT: The version of the FastAPI application. [default: 1.0.0]</li> <li><code>-p</code>, <code>--port</code> INTEGER: The port to bind the server to. [default: 8000]</li> <li><code>-H</code>, <code>--host</code> TEXT: The host to bind the server to. [default: 0.0.0.0]</li> <li><code>-x</code>, <code>--strict</code>: If True, the application will not start if any table configuration is invalid. [default: False]</li> <li><code>-m</code>, <code>--max-count</code> INTEGER: Maximum number of records that can be requested in a single API call. [default: 1000]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>Mimicry uses YAML files to define how data should be generated (table configuration) and where it should be sent (sink configuration).</p>"},{"location":"configuration/#table-configuration","title":"Table Configuration","text":"<p>A table configuration file defines the schema of the data to be generated. It specifies the table's name, description, locale for data generation, and a list of fields.</p> <p>Example <code>my_table_schema.yaml</code>:</p> <pre><code>name: DimEmployees\nlocale: en\ndescription: Dimension table for employee information.\nfields:\n- description: Unique identifier for the employee.\n  mimesis_field_args: []\n  mimesis_field_name: person.identifier # Placeholder, adjust with actual mimesis field\n  name: employee_id\n- description: National identification number for the employee.\n  mimesis_field_args: []\n  mimesis_field_name: person.identifier # Placeholder, adjust with actual mimesis field\n  name: national_id\n- description: Full name of the employee.\n  mimesis_field_args: []\n  mimesis_field_name: person.full_name\n  name: full_name\n- description: Job title or position held by the employee.\n  mimesis_field_args: []\n  mimesis_field_name: person.occupation\n  name: job_title\n- description: Work email address of the employee.\n  mimesis_field_args: []\n  mimesis_field_name: person.email\n  name: email\n- description: Date when the employee was hired.\n  mimesis_field_args: []\n  mimesis_field_name: datetime.date\n  name: hire_date\n- description: Annual salary of the employee in USD.\n  mimesis_field_args: []\n  mimesis_field_name: finance.price # Placeholder, adjust with actual mimesis field\n  name: salary\n- description: Employee ID of the manager (foreign key).\n  mimesis_field_args: []\n  mimesis_field_name: person.identifier # Placeholder, adjust with actual mimesis field\n  name: manager_id_fk\n- description: City where the employee's office is located.\n  mimesis_field_args: []\n  mimesis_field_name: address.city\n  name: office_location\n- description: Indicates if the employee is currently active.\n  mimesis_field_args: []\n  mimesis_field_name: development.boolean\n  name: is_active\n</code></pre> <p>Please refer to the Mimesis documentation for a complete list of available fields and their parameters.</p> <p>Fields in <code>TableConfiguration</code>:</p> <ul> <li><code>name</code> (str): The name of the table.</li> <li><code>description</code> (str): A description of the table.</li> <li><code>locale</code> (str, optional): The Mimesis locale to use for data generation (e.g., \"en\", \"de\", \"ja\"). Defaults to \"en\".</li> <li><code>fields</code> (list): A list of <code>FieldConfiguration</code> objects.</li> </ul> <p>Fields in <code>FieldConfiguration</code>:</p> <ul> <li><code>name</code> (str): The name of the field (column name).</li> <li><code>description</code> (str): A description of the field.</li> <li><code>mimesis_field_name</code> (str): The Mimesis provider and method to use (e.g., \"person.full_name\", \"address.city\", \"numeric.float_number\").</li> <li><code>mimesis_field_args</code> (list, optional): A list of positional arguments to pass to the Mimesis method.</li> <li><code>mimesis_field_kwargs</code> (dict, optional): A dictionary of keyword arguments to pass to the Mimesis method.</li> </ul>"},{"location":"configuration/#sink-configuration","title":"Sink Configuration","text":"<p>A sink configuration file defines where the generated data should be stored or sent.</p> <p>Example <code>my_sink_config.yaml</code>:</p> <pre><code>configuration:\n  type_of_sink: \"delta_lake\"  # or \"duckdb\", \"postgres\", \"kafka\", \"iceberg\"\n  # Sink-specific configuration below\n  path: \"/path/to/delta_lake_table\" # Example for delta_lake\n  table_name: \"users_table\" # Example for duckdb, postgres, iceberg\n  # producer_config: {} # Example for kafka\n  # topic: \"my_topic\" # Example for kafka\n  # catalog_properties: {} # Example for iceberg\n</code></pre> <p>The top-level key is <code>configuration</code>, which then contains the specific sink type and its parameters. Refer to the Sinks documentation for details on each sink type.</p>"},{"location":"configuration/#support-for-configurations-stored-in-the-object-storage","title":"Support for configurations stored in the object storage","text":"<p>Mimicry supports configurations stored in object storage systems like Google Cloud Storage (GCS) and Amazon S3. You can specify the path to the configuration file in the object storage, and Mimicry will handle loading it.</p>"},{"location":"configuration/#example-for-gcs","title":"Example for GCS","text":"<pre><code>uv run mimicry serve --schema gs://bucket/schemas/my_table_schema.yaml\nuv run mimicry generate --schema gs://bucket/schemas/traffic.yaml --sink gs://bucket/sinks/deltalake_sink.yaml -i 1 -c 10000 -b 15\n</code></pre>"},{"location":"configuration/#example-for-s3","title":"Example for S3","text":"<pre><code>uv run mimicry serve --schema s3://bucket/schemas/my_table_schema.yaml\nuv run mimicry generate --schema s3://bucket/schemas/traffic.yaml --sink s3://bucket/sinks/deltalake_sink.yaml -i 1 -c 10000 -b 15\n</code></pre>"},{"location":"sinks/","title":"Supported Sinks","text":"<p>Mimicry can stream generated data to various storage solutions.</p>"},{"location":"sinks/#delta-lake-sink","title":"Delta Lake Sink","text":"<ul> <li><code>type_of_sink</code> (<code>\"delta_lake\"</code>): Must be \"delta_lake\".</li> <li><code>path</code> (str): The path to the Delta Lake table directory.</li> <li><code>partition_by</code> (list[str], optional): A list of column names to partition the Delta table by.</li> <li><code>optimize</code> (int, optional): If set, performs a COMPACT optimization on the Delta table every N batches.</li> <li><code>vacuum</code> (int, optional): If set, vacuums the Delta table every N batches to remove old, unreferenced files.</li> </ul>"},{"location":"sinks/#local-file-system-example","title":"Local File System Example","text":"<p>Example:</p> <pre><code>configuration:\n  type_of_sink: \"delta_lake\"\n  path: \"/mnt/data/my_delta_table\"\n  partition_by: [\"year\", \"month\"]\n  optimize: 10\n  vacuum: 20\n</code></pre>"},{"location":"sinks/#amazon-s3-example","title":"Amazon S3 Example","text":"<p>Delta Lake supports Amazon S3 using the <code>s3://</code> URI scheme. Authentication is handled via environment variables. Please check object_store docs for more.</p> <p>Configuration:</p> <pre><code>configuration:\n  type_of_sink: \"delta_lake\"\n  path: \"s3://${S3_BUCKET_NAME}/delta-tables/employees\"\n  partition_by: [\"year\", \"month\", \"day\"]\n  optimize: 10\n  vacuum: 20\n</code></pre> <p>Required Environment Variables:</p> <pre><code># AWS Credentials\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_REGION=us-east-1\n</code></pre> <p>Optional Environment Variables:</p> <pre><code># For temporary credentials (IAM roles, etc.)\nexport AWS_SESSION_TOKEN=your_session_token\n\n# For S3-compatible storage (MinIO, etc.)\nexport AWS_ENDPOINT_URL=https://your-s3-compatible-endpoint\n</code></pre>"},{"location":"sinks/#google-cloud-storage-example","title":"Google Cloud Storage Example","text":"<p>Delta Lake supports Google Cloud Storage using the <code>gs://</code> URI scheme. Authentication is handled via environment variables. Please check object_store docs for more.</p> <p>Configuration:</p> <pre><code>configuration:\n  type_of_sink: \"delta_lake\"\n  path: \"gs://bucket_name/delta-tables/employees\"\n  partition_by: [\"year\", \"month\", \"day\"]\n  optimize: 10\n  vacuum: 20\n</code></pre> <p>Authentication Methods:</p> <ol> <li>Service Account Key (Recommended for Production):</li> </ol> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json\n</code></pre> <p>Check GOOGLE_APPLICATION_CREDENTIALS environment variable for more</p> <ol> <li>Application Default Credentials (Development):</li> </ol> <pre><code># Login with gcloud CLI\ngcloud auth application-default login\n</code></pre> <ol> <li>Workload Identity (Kubernetes):    No additional environment variables needed if configured properly in your Kubernetes cluster.</li> </ol> <p>Optional Environment Variables:</p> <pre><code># Google Cloud Project\nexport GOOGLE_CLOUD_PROJECT=your-project-id\n</code></pre> <p>Docker Example:</p> <pre><code># Mounting Service Account JSON File Key\ndocker run --rm \\\n  -v /path/to/secret_file.json:/secrets/gcp-key.json:ro \\\n  -e GOOGLE_APPLICATION_CREDENTIALS=/secrets/gcp-key.json \\\n  -e GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT_ID \\\n  mimicry:latest \\\n    uv run mimicry generate \\\n    --schema examples/tables/DimGeospatialData.yaml \\\n    --sink examples/sinks/deltalake_gcs_sink.yaml \\\n    --count 1000 \\\n    --interval 2 \\\n    --batches 10\n</code></pre>"},{"location":"sinks/#duckdb-sink","title":"DuckDB Sink","text":"<p>DuckDB does not support read and write operations by multiple processes simultaneously.</p> <p>This means that if you are generating data to a DuckDB sink, ensure that no other processes are trying to read from or write to the same DuckDB database file at the same time.</p> <p>MIMICRY_UNSAFE_DUCKDB environment variable</p> <p>If you set the <code>MIMICRY_UNSAFE_DUCKDB</code> environment variable to <code>true</code> mimicry will create the DuckDB table if missing. Note that this is unsafe, if the table name is not sanitized, it may lead to SQL injection vulnerabilities.</p> <ul> <li><code>type_of_sink</code> (<code>\"duckdb\"</code>): Must be \"duckdb\".</li> <li><code>path</code> (str): The path to the DuckDB database file.</li> <li><code>table_name</code> (str): The name of the table within the DuckDB database where data will be appended.</li> </ul> <p>Example:</p> <pre><code>configuration:\n  type_of_sink: \"duckdb\"\n  path: \"my_database.db\"\n  table_name: \"raw_events\"\n</code></pre>"},{"location":"sinks/#postgresql-sink","title":"PostgreSQL Sink","text":"<ul> <li><code>type_of_sink</code> (<code>\"postgres\"</code>): Must be \"postgres\".</li> <li><code>connection_string</code> (str): The SQLAlchemy connection string for the PostgreSQL database.</li> <li><code>table_name</code> (str): The name of the table within the PostgreSQL database where data will be appended.</li> </ul> <p>Example:</p> <pre><code>configuration:\n  type_of_sink: \"postgres\"\n  connection_string: \"postgresql://user:password@host:port/database\"\n  table_name: \"fact_orders\"\n</code></pre>"},{"location":"sinks/#kafka-sink","title":"Kafka Sink","text":"<p>Kafka Sink is experimental and may not be fully functional.</p> <p>It might not be the most efficient way to stream data to Kafka.</p> <ul> <li><code>type_of_sink</code> (<code>\"kafka\"</code>): Must be \"kafka\".</li> <li><code>producer_config</code> (dict): Configuration for the Kafka producer. Please refer to the kafka-python documentation for available options.</li> <li><code>topic</code> (str): The Kafka topic to which messages will be sent.</li> </ul> <p>Example:</p> <pre><code>configuration:\n  type_of_sink: \"kafka\"\n  producer_config:\n    bootstrap_servers: \"kafka1:9092,kafka2:9092\"\n    client_id: \"mimicry-producer\"\n  topic: \"user_activity_stream\"\n</code></pre>"},{"location":"sinks/#iceberg-sink","title":"Iceberg Sink","text":"<p>Iceberg Sink is experimental and may not be fully functional.</p> <p>It was tested only with the REST catalog and may not work with other catalog types.</p> <ul> <li><code>type_of_sink</code> (<code>\"iceberg\"</code>): Must be \"iceberg\".</li> <li><code>table_name</code> (str): The fully qualified name of the Iceberg table (e.g., <code>nessie.db.my_table</code>).</li> <li><code>catalog_properties</code> (dict[str, str]): Properties to configure the Iceberg catalog. This typically includes settings for the catalog type (e.g., REST, Hive, Nessie), URI, warehouse location, and any authentication details.</li> </ul> <p>Example (REST Catalog):</p> <pre><code>configuration:\n  type_of_sink: \"iceberg\"\n  table_name: \"my_catalog.my_schema.my_iceberg_table\"\n  catalog_properties:\n    uri: \"http://localhost:8181\"\n    s3.endpoint: \"http://minio:9000\"\n    s3.access-key-id: \"minioadmin\"\n    s3.secret-access-key: \"minioadmin\"\n    # Add other necessary properties for your specific catalog\n</code></pre>"}]}